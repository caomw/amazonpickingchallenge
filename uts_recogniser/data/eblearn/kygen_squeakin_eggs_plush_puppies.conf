################################################################################
# important training parameters (others are described later below)
eta=.000005 	     		# learning rate
epoch_mode=1	     # 0: fixed number of samples/iter 1: show all at least once
iterations=20      # number of training iterations

# training data ################################################################
root=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/ 	                # root directory of the dataset
train_dsname=kygen_squeakin_eggs_plush_puppies+bg_train   # name of train dataset
val_dsname=kygen_squeakin_eggs_plush_puppies+bg_val 	# name of validation dataset

train=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_train_data.mat
train_labels=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_train_labels.mat
train_classes=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_train_classes.mat
# train_size = 1000            # limit number of samples

val=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_val_data.mat
val_labels=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_val_labels.mat
val_classes=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_val_classes.mat
# val_size = 100                # limit number of samples


# network ######################################################################
classification=1                # 1 is classification, 0 is regression
input_thickness=3               # input has 3-channels
#arch = ${pp_${run_type}},${c0},${s1},${c2},${s3},${c4},${f7} # global architecture
arch = resizepp0,conv0,addc0,tanh0,abs0,wstd0,subs1,addc1,tanh1,conv2,addc2,tanh2,abs2,wstd2,subs3,addc3,tanh3,conv4,addc4,tanh4,linear7,addc7,tanh7 # global architecture
nonlin = tanh  					       # type of nonlinearity to use


# layers
c0 = conv0,addc0,tanh0,abs0,wstd0
s1 = subs1,addc1,tanh1
c2 = conv2,addc2,tanh2,abs2,wstd2
s3 = subs3,addc3,tanh3
#c4 = conv4,addc4,tanh4,abs4,wstd4

c4 = conv4,addc4,tanh4
f5 = linear5,addc5,tanh5

f7 = linear7,addc7,tanh7

# energies & answers ###########################################################
trainer=trainable_module1
trainable_module1_energy=l2_energy
answer=class_answer
background_name = bg

# preprocessing modules
pp_yp = rgb_to_ypuv0
rgb_to_ypuv0_kernel = 7x7
resizepp0_pp = rgb_to_ypuv0
pp_train = #rgb_to_ypuv0 #mschan0
pp_detect = resizepp0

# modules parameters
inputh = 64				# input's height
inputw = 64 				# input's width
conv0_kernel = 9x9 			# convolution kernel
conv0_stride = 1x1 			# convolution stride
conv0_table =  				# convolution table (optional)
conv0_table_in = 3 			# conv input max, used if table file not defined
conv0_table_out = 6 			# features max, used if table file not defined
wstd0_kernel = 5x5 			# normalization kernel
subs1_kernel = 2x2 			# subsampling kernel
subs1_stride = 2x2 #${subs1_kernel} 		# subsampling stride
addc1_weights = 			# weights to be loaded if manual_load = 1
conv2_kernel = 9x9 			# convolution kernel
conv2_stride = 1x1 			# convolution stride
conv2_table = /home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/table_6_16_connect_60.mat 		# convolution table (optional)
conv2_table_in = thickness 		# use current thickness as max table input
conv2_table_out = ${table1_max} 	# features max, used if table file not defined
conv2_weights =   
addc2_weights = 
diag2_weights = 
wstd2_kernel = 5x5			# normalization kernel
subs3_kernel = 2x2 			# subsampling kernel
subs3_stride = 2x2 # ${subs3_kernel} 		# subsampling stride
addc3_weights = 			# weights to be loaded if manual_load = 1
conv4_kernel = 10x10			# convolution kernel
conv4_stride = 1x1 			# convolution stride
conv4_table_in = thickness 		# use current thickness as max table input
conv4_table_out = 100 			# use number of classes as max table output
linear5_in = thickness   		# linear module input features size
linear5_out = noutputs 			# use number of classes as max table output

linear7_in      = thickness # use current thickness
linear7_out     = noutputs # use number of classes as max table output

# tables #######################################################################
table0_max = 6 # full table output max (overridden if table file defined)
table1 = /home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/table_6_16_connect_60.mat

# preprocessing ################################################################
resize=mean
normalization_size=7

# training params ##############################################################
reg_l1=${reg} # L1 regularization
reg_l2=${reg} # L2 regularization

inertia = 0.0 # gradient inertia
anneal_value = 0.0 # learning rate decay value
annea_period = 0 # period (in samples) at which to decay learning rate
gradient_threshold = 0.0

epoch_show_modulo=1000    # print message every n training samples
no_testing_test=0 	  # do not test on test set if 1
no_training_test=1 	  # do not test on training set if 1
test_only=0 		  # if 1, just test the data and return
sample_probabilities=0 	  # use probabilities to pick samples
hardest_focus=0           # 0: focus on easiest samples 1: focus on hardest ones
ignore_correct=0 	  # If 1, do not train on correctly classified samples
min_sample_weight=0 	  # minimum probability of each sample
per_class_norm=1 	  # normalize probability by class (1) or globally (0)
shuffle_passes=1 	  # shuffle samples between passes
balanced_training=1 	  # show each class the same amount of samples or not
random_class_order=1 	  # class order is randomized or not when balanced
target_factor=1
save_pickings=0		  # save sample picking statistics
binary_target=0           # use only 1 output, -1 or +1
save_weights=1  	  # if 0, do not save weights after each iteration
save_confusion=0 	  # if 0, do not save confusion matrix after each iter
keep_outputs=0 		  # keep the predicted outputs in this iteration
fixed_randomization=1 	  # if 1, uses a fixed randomization seed
training_precision=double # training precision

# tracking ####################################################################
mainsleep = 5
smooth_factor = 1.0
# display tracking
tracking_display = 0

# training display #############################################################
display               = 1  # display results
show_conf             = 1  # show configuration variables or not
show_train            = 0  # enable/disable all training display
show_train_ninternals = 1  # number of internal examples to display
show_train_errors     = 0  # show worst errors on training set
show_train_correct    = 0  # show worst corrects on training set
show_val_errors       = 1  # show worst errors on validation set
show_val_correct      = 1  # show worst corrects on validation set
show_hsample          = 5  # number of samples to show on height axis
show_wsample          = 18 # number of samples to show on height axis
show_wait_user        = 0  # if 1, wait for user to close windows

# detection ####################################################################
weights=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies.mat   #${root2}/trained/20100227.175046.face_conf05_eta_.000005_retrain_1_net044.mat
classes=/home/kanzhi/hydro_workspace/amazon_picking_challenge/uts_recogniser/data/eblearn/models/kygen_squeakin_eggs_plush_puppies+bg_val_classes.mat    #${root2}/trained/20100227.175046.face_conf05_eta_.000005_retrain_1_classes.mat
input_dir = test
#input_list = 396.jpg
#show_conf = 1
input_height=480     	#-1#120#480
#input_width=640		#-1#160#640
#input_min=100
input_max=800
# multi-scaling type. 0: manually set each scale sizes, 1: manually set each
# scale step, 2: number of scales between min and max, 3: step factor between
# min and max, 4: 1 scale, the original image size.
scaling_type = 3
# scaling ratio between scales
scaling = 0.2
# scale factor of maximum resolution of the original resolution
max_scale = 0.5
# scale factor of minimum resolution of the original resolution
min_scale = 0.2
# number of detection threads
nthreads = 1
# randomize image input list (only works for 'directory' camera).
input_random = 0
# number of passes on the image input list (only works for 'directory' camera).
input_npasses = 1
# height factor to apply to bounding boxes
bbhfactor = 1
# width factor to apply to bounding boxes
bbwfactor = 1
# prune overlapping bounding boxes or not
pruning = 1
# minimum height ratio with smallest bbox to declare overlap
bbh_overlap = .67
# minimum width ratio with smallest bbox to declare overlap
bbw_overlap = 0
hzpad = .5 # vertical zero padding on each side
wzpad = .5 # horizontal zero padding on each side
mem_optimization = 1
confidence_type = 2 # 0: sqrdist 1: single output 2: max other (recommended)

# detection display ############################################################
# output saving and display
output_dir = ./result
detection_dir = /detect
bbox_saving = 2
save_bbox_period = 1
# save each classified frame and make a video out of it
save_video=0
# fps at which video should be constructed
save_video_fps=0
# if loaded a video and equal to 1, reuse video's fps
use_original_fps=0
# enable or disable display
display = 1
display_threads = 1 # each thread displays on its own
# only show classified input
minimal_display = 0
silent = 0 # minimize outputs to be printed
sync_outputs = 0 # synchronize output between threads
# display internal states of network
display_states = 0
display_min = -1.7
display_max = 1.7
display_in_min = 0 # input image min display range
display_in_max = 255 # input image max display range
display_bb_transparency = 0 #0.3
display_sleep = 100000 # sleep in milliseconds after displaying
ninternals=1

# camera options: v4l2 opencv shmem video directory
camera = directory # v4l2 #opencv # directory

threshold=0.8
pre_threshold = -1
post_threshold = 0.8 #${threshold}
nms = 3
pre_hfact             = 1   # bbox height factor before nms
pre_wfact             = 1   # bbox width factor before nms
post_hfact            = 1   # bbox height factor after nms
post_wfact            = 1   # bbox width factor after nms
woverh                = 1   # set width to be h * woverh if different than 1.0
max_hcenter_dist      = .3  # regular, bboxes match when hcenter dist below this
max_wcenter_dist      = .3  # regular, bboxes match when wcenter dist below this
vote_max_overlap      = .5  # voting, bboxes match when overlap less than this
vote_max_hcenter_dist = .5  # voting, match when hcenter dist below this
vote_max_wcenter_dist = .3  # voting, match when wcenter dist below this

display_sleep = 800 # sleep in milliseconds after displaying
scaling = 1.2  #2.2 1.9 1.3 1.2 1.1
# multi-scaling type. 0: manually set each scale sizes, 1: manually set each
# scale step, 2: number of scales between min and max, 3: step factor between
# min and max, 4: 1 scale, the original image size.
scaling_type = 3
max_scale = 0.4
min_scale = 1.2
#input_max = 288
show_extracted = 2


